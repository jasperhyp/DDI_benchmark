{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nih492/benchmark_nddi/CASTER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n",
      "reserved memory 0\n",
      "allocated memory 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('./DDE')\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch import nn \n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import KFold\n",
    "torch.manual_seed(2)    # reproducible torch:2 np:3\n",
    "np.random.seed(3)\n",
    "\n",
    "from dde_config import dde_NN_config\n",
    "from dde_torch import dde_NN_Large_Predictor\n",
    "from stream_dde import supData, unsupData\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "# print(\"total memory\", t)\n",
    "print(torch.cuda.get_device_properties('cuda:0'))\n",
    "print(\"reserved memory\", r)\n",
    "print(\"allocated memory\", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "--- Data Preparation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.parallel.data_parallel.DataParallel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'dde_torch.dde_NN_Large_Predictor' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/tmp/ipykernel_10625/2502797118.py:41: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_unsup = pd.read_csv('data/unsup_dataset.csv', names = ['idx', 'input1_SMILES', 'input2_SMILES', 'type']).drop(0)# pairs dataframe input1_smiles, input2_smiles\n"
     ]
    }
   ],
   "source": [
    "config = dde_NN_config()\n",
    "pretrain_epoch = config['pretrain_epoch']\n",
    "pretrain_epoch = 0\n",
    "train_epoch = 9\n",
    "lr = config['LR']\n",
    "thr = config['recon_threshold']\n",
    "recon_loss_coeff = config['reconstruction_coefficient']\n",
    "proj_coeff = config['projection_coefficient']\n",
    "lambda1 = config['lambda1']\n",
    "lambda2 = config['lambda2']\n",
    "BATCH_SIZE = config['batch_size']\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "loss_r_history = []\n",
    "loss_p_history = []\n",
    "loss_c_history = []\n",
    "loss_history = []\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "path = 'model_pretrain_checkpoint_1.pt'\n",
    "model_nn = torch.load(path)\n",
    "model_nn.cuda()\n",
    "\n",
    "mod = model_nn.module\n",
    "model_nn = nn.DataParallel(mod, device_ids=[0, 1])\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(model_nn.parameters(), lr = lr)\n",
    "\n",
    "print('--- Data Preparation ---')\n",
    "\n",
    "params = {'batch_size': BATCH_SIZE,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 3}\n",
    "\n",
    "# dataFolder = 'DDE/data'\n",
    "\n",
    "df_unsup = pd.read_csv('data/unsup_dataset.csv', names = ['idx', 'input1_SMILES', 'input2_SMILES', 'type']).drop(0)# pairs dataframe input1_smiles, input2_smiles\n",
    "df_ddi = pd.read_csv('data/BIOSNAP/sup_train_val.csv')  # ddi dataframe drug1_smiles, drug2_smiles\n",
    "\n",
    "#5-fold\n",
    "kf = KFold(n_splits = 8, shuffle = True, random_state = 3)\n",
    "#get the 1st fold index\n",
    "fold_index = next(kf.split(df_ddi), None)\n",
    "\n",
    "ids_unsup = df_unsup.index.values\n",
    "partition_sup = {'train': fold_index[0], 'val': fold_index[1]}\n",
    "labels_sup = df_ddi.label.values\n",
    "\n",
    "unsup_set = unsupData(ids_unsup, df_unsup)\n",
    "unsup_generator = data.DataLoader(unsup_set, **params)\n",
    "\n",
    "training_set = supData(partition_sup['train'], labels_sup, df_ddi)\n",
    "training_generator_sup = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = supData(partition_sup['val'], labels_sup, df_ddi)\n",
    "validation_generator_sup = data.DataLoader(validation_set, **params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147\n",
      "9328\n",
      "1204\n",
      "9306\n",
      "33243\n",
      "33189\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_heads = df_ddi[df_ddi[\"label\"] == 1.0].groupby([\"Drug1_SMILES\"]).count()\n",
    "neg_heads = df_ddi[df_ddi[\"label\"] == 0.0].groupby([\"Drug1_SMILES\"]).count()\n",
    "\n",
    "# just the number of unique drogs in the positives\n",
    "print(len(pos_heads))\n",
    "print(len(neg_heads)) ## there appears to be a lot more different negative drugs\n",
    "\n",
    "\n",
    "pos_tails = df_ddi[df_ddi[\"label\"] == 1.0].groupby([\"Drug2_SMILES\"]).count()\n",
    "neg_tails = df_ddi[df_ddi[\"label\"] == 0.0].groupby([\"Drug2_SMILES\"]).count()\n",
    "\n",
    "print(len(pos_tails))\n",
    "print(len(neg_tails)) ## there appears to be a lot more different negative drugs\n",
    "\n",
    "\n",
    "## equal number of positive and negative data points\n",
    "print(len(df_ddi[df_ddi[\"label\"] == 1.0]))\n",
    "print(len(df_ddi[df_ddi[\"label\"] == 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305\n",
      "9645\n"
     ]
    }
   ],
   "source": [
    "## how many unique drugs are there for positive labels\n",
    "pos = df_ddi[df_ddi[\"label\"] == 1.0]\n",
    "pos_smiles = set(pos[\"Drug1_SMILES\"].unique()) | set(pos[\"Drug2_SMILES\"].unique())\n",
    "print(len(pos_smiles))\n",
    "\n",
    "neg = df_ddi[df_ddi[\"label\"] == 0.0]\n",
    "neg_smiles = set(neg[\"Drug1_SMILES\"].unique()) | set(neg[\"Drug2_SMILES\"].unique())\n",
    "print(len(neg_smiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Drug1_ID</th>\n",
       "      <th>Drug1_SMILES</th>\n",
       "      <th>Drug2_ID</th>\n",
       "      <th>Drug2_SMILES</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DB00706</td>\n",
       "      <td>CCOc1ccccc1OCCN[C@H](C)Cc1ccc(OC)c(S(N)(=O)=O)c1</td>\n",
       "      <td>DB01023</td>\n",
       "      <td>CCOC(=O)C1=C(C)NC(C)=C(C(=O)OC)C1c1cccc(Cl)c1Cl</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>DB00802</td>\n",
       "      <td>CCC(=O)N(c1ccccc1)C1(COC)CCN(CCn2nnn(CC)c2=O)CC1</td>\n",
       "      <td>DB00872</td>\n",
       "      <td>Cc1nc2c([nH]1)CCN(C(=O)c1ccc(NC(=O)c3ccccc3-c3...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>DB00912</td>\n",
       "      <td>CCOc1cc(CC(=O)N[C@@H](CC(C)C)c2ccccc2N2CCCCC2)...</td>\n",
       "      <td>DB01238</td>\n",
       "      <td>O=C1CCc2ccc(OCCCCN3CCN(c4cccc(Cl)c4Cl)CC3)cc2N1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>DB00601</td>\n",
       "      <td>CC(=O)NC[C@H]1CN(c2ccc(N3CCOCC3)c(F)c2)C(=O)O1</td>\n",
       "      <td>DB01104</td>\n",
       "      <td>CN[C@H]1CC[C@@H](c2ccc(Cl)c(Cl)c2)c2ccccc21</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>DB00370</td>\n",
       "      <td>CN1CCN2c3ncccc3Cc3ccccc3C2C1</td>\n",
       "      <td>DB08881</td>\n",
       "      <td>CCCS(=O)(=O)Nc1ccc(F)c(C(=O)c2c[nH]c3ncc(-c4cc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66424</th>\n",
       "      <td>66424</td>\n",
       "      <td>DB00622</td>\n",
       "      <td>COC(=O)C1=C(C)NC(C)=C(C(=O)OCCN(C)Cc2ccccc2)C1...</td>\n",
       "      <td>DB08933</td>\n",
       "      <td>N#C/C(=C1/SC[C@@H](c2ccc(Cl)cc2Cl)S1)n1ccnc1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66425</th>\n",
       "      <td>66425</td>\n",
       "      <td>DB01233</td>\n",
       "      <td>CCN(CC)CCNC(=O)c1cc(Cl)c(N)cc1OC</td>\n",
       "      <td>DB01463</td>\n",
       "      <td>CCNC1C2CCC(C2)C1c1ccccc1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66427</th>\n",
       "      <td>66427</td>\n",
       "      <td>DB01319</td>\n",
       "      <td>CC(C)CN(C[C@@H](OP(=O)(O)O)[C@H](Cc1ccccc1)NC(...</td>\n",
       "      <td>DB08882</td>\n",
       "      <td>CC#CCn1c(N2CCC[C@@H](N)C2)nc2c1c(=O)n(Cc1nc(C)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66428</th>\n",
       "      <td>66428</td>\n",
       "      <td>DB00295</td>\n",
       "      <td>CN1CC[C@]23c4c5ccc(O)c4O[C@H]2[C@@H](O)C=C[C@H...</td>\n",
       "      <td>DB09280</td>\n",
       "      <td>Cc1ccc(NC(=O)C2(c3ccc4c(c3)OC(F)(F)O4)CC2)nc1-...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66430</th>\n",
       "      <td>66430</td>\n",
       "      <td>DB00956</td>\n",
       "      <td>COc1ccc2c3c1O[C@H]1C(=O)CC[C@H]4[C@@H](C2)N(C)...</td>\n",
       "      <td>DB01202</td>\n",
       "      <td>CC[C@@H](C(N)=O)N1CCCC1=O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33243 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 Drug1_ID                                       Drug1_SMILES  \\\n",
       "0               0  DB00706   CCOc1ccccc1OCCN[C@H](C)Cc1ccc(OC)c(S(N)(=O)=O)c1   \n",
       "4               4  DB00802   CCC(=O)N(c1ccccc1)C1(COC)CCN(CCn2nnn(CC)c2=O)CC1   \n",
       "5               5  DB00912  CCOc1cc(CC(=O)N[C@@H](CC(C)C)c2ccccc2N2CCCCC2)...   \n",
       "6               6  DB00601     CC(=O)NC[C@H]1CN(c2ccc(N3CCOCC3)c(F)c2)C(=O)O1   \n",
       "10             10  DB00370                       CN1CCN2c3ncccc3Cc3ccccc3C2C1   \n",
       "...           ...      ...                                                ...   \n",
       "66424       66424  DB00622  COC(=O)C1=C(C)NC(C)=C(C(=O)OCCN(C)Cc2ccccc2)C1...   \n",
       "66425       66425  DB01233                   CCN(CC)CCNC(=O)c1cc(Cl)c(N)cc1OC   \n",
       "66427       66427  DB01319  CC(C)CN(C[C@@H](OP(=O)(O)O)[C@H](Cc1ccccc1)NC(...   \n",
       "66428       66428  DB00295  CN1CC[C@]23c4c5ccc(O)c4O[C@H]2[C@@H](O)C=C[C@H...   \n",
       "66430       66430  DB00956  COc1ccc2c3c1O[C@H]1C(=O)CC[C@H]4[C@@H](C2)N(C)...   \n",
       "\n",
       "      Drug2_ID                                       Drug2_SMILES  label  \n",
       "0      DB01023    CCOC(=O)C1=C(C)NC(C)=C(C(=O)OC)C1c1cccc(Cl)c1Cl    1.0  \n",
       "4      DB00872  Cc1nc2c([nH]1)CCN(C(=O)c1ccc(NC(=O)c3ccccc3-c3...    1.0  \n",
       "5      DB01238    O=C1CCc2ccc(OCCCCN3CCN(c4cccc(Cl)c4Cl)CC3)cc2N1    1.0  \n",
       "6      DB01104        CN[C@H]1CC[C@@H](c2ccc(Cl)c(Cl)c2)c2ccccc21    1.0  \n",
       "10     DB08881  CCCS(=O)(=O)Nc1ccc(F)c(C(=O)c2c[nH]c3ncc(-c4cc...    1.0  \n",
       "...        ...                                                ...    ...  \n",
       "66424  DB08933       N#C/C(=C1/SC[C@@H](c2ccc(Cl)cc2Cl)S1)n1ccnc1    1.0  \n",
       "66425  DB01463                           CCNC1C2CCC(C2)C1c1ccccc1    1.0  \n",
       "66427  DB08882  CC#CCn1c(N2CCC[C@@H](N)C2)nc2c1c(=O)n(Cc1nc(C)...    1.0  \n",
       "66428  DB09280  Cc1ccc(NC(=O)C2(c3ccc4c(c3)OC(F)(F)O4)CC2)nc1-...    1.0  \n",
       "66430  DB01202                          CC[C@@H](C(N)=O)N1CCCC1=O    1.0  \n",
       "\n",
       "[33243 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = neg_smiles - pos_smiles\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (v_D, label) in enumerate(training_generator_sup):\n",
    "    break\n",
    "    \n",
    "v_D = v_D.float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_nn(v_D)\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_dde_nn(data_generator, model_nn):\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    model_nn.eval()\n",
    "    for i, (v_D, label) in enumerate(data_generator):\n",
    "        recon, code, score, Z_f, z_D = model_nn(v_D.float().cuda())\n",
    "        m = torch.nn.Sigmoid()\n",
    "        logits = torch.squeeze(m(score)).detach().cpu().numpy()\n",
    "        label_ids = label.to('cpu').numpy()\n",
    "        y_label = y_label + label_ids.flatten().tolist()\n",
    "        y_pred = y_pred + logits.flatten().tolist()\n",
    "\n",
    "    return roc_auc_score(y_label, y_pred), y_pred\n",
    "\n",
    "def main_dde_nn():\n",
    "    config = dde_NN_config()\n",
    "    pretrain_epoch = config['pretrain_epoch']\n",
    "    pretrain_epoch = 0\n",
    "    train_epoch = 9\n",
    "    lr = config['LR']\n",
    "    thr = config['recon_threshold']\n",
    "    recon_loss_coeff = config['reconstruction_coefficient']\n",
    "    proj_coeff = config['projection_coefficient']\n",
    "    lambda1 = config['lambda1']\n",
    "    lambda2 = config['lambda2']\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    BATCH_SIZE = 256 #256\n",
    "    \n",
    "    loss_r_history = []\n",
    "    loss_p_history = []\n",
    "    loss_c_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    # model_nn = dde_NN_Large_Predictor(**config)\n",
    "    path = 'model_pretrain_checkpoint_1.pt'\n",
    "    model_nn = torch.load(path)\n",
    "    model_nn.cuda()\n",
    "\n",
    "    print(torch.cuda.device_count()) # 2\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        mod = model_nn.module\n",
    "        model_nn = nn.DataParallel(mod, device_ids=[0, 1])\n",
    "        \n",
    "        \n",
    "    opt = torch.optim.Adam(model_nn.parameters(), lr = lr)\n",
    "    \n",
    "    print('--- Data Preparation ---')\n",
    "    \n",
    "    params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 3}\n",
    "\n",
    "    # dataFolder = 'DDE/data'\n",
    "\n",
    "    df_unsup = pd.read_csv('data/unsup_dataset.csv', names = ['idx', 'input1_SMILES', 'input2_SMILES', 'type']).drop(0)# pairs dataframe input1_smiles, input2_smiles\n",
    "    df_ddi = pd.read_csv('data/BIOSNAP/sup_train_val.csv')  # ddi dataframe drug1_smiles, drug2_smiles\n",
    "\n",
    "    #5-fold\n",
    "    kf = KFold(n_splits = 8, shuffle = True, random_state = 3)\n",
    "    #get the 1st fold index\n",
    "    fold_index = next(kf.split(df_ddi), None)\n",
    "\n",
    "    ids_unsup = df_unsup.index.values\n",
    "    partition_sup = {'train': fold_index[0], 'val': fold_index[1]}\n",
    "    labels_sup = df_ddi.label.values\n",
    "\n",
    "    unsup_set = unsupData(ids_unsup, df_unsup)\n",
    "    unsup_generator = data.DataLoader(unsup_set, **params)\n",
    "\n",
    "    training_set = supData(partition_sup['train'], labels_sup, df_ddi)\n",
    "    training_generator_sup = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = supData(partition_sup['val'], labels_sup, df_ddi)\n",
    "    validation_generator_sup = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "    print(len(training_generator_sup))\n",
    "    \n",
    "    max_auc = 0\n",
    "    model_max = copy.deepcopy(model_nn)\n",
    "    \n",
    "    print('--- Pre-training Starts ---')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    len_unsup = len(unsup_generator)\n",
    "    for pre_epo in range(pretrain_epoch):\n",
    "        for i, v_D in enumerate(unsup_generator):\n",
    "            v_D = v_D.float().cuda()\n",
    "            recon, code, score, Z_f, z_D = model_nn.module(v_D)\n",
    "            loss_r = recon_loss_coeff * F.binary_cross_entropy(recon, v_D.float())\n",
    "            \n",
    "            loss_p = proj_coeff * (torch.norm(z_D - torch.matmul(code, Z_f)) + lambda1 * torch.sum(torch.abs(code)) / BATCH_SIZE + lambda2 * torch.norm(Z_f, p='fro') / BATCH_SIZE)\n",
    "            loss = loss_r + loss_p\n",
    "            \n",
    "            loss_r_history.append(loss_r)\n",
    "            loss_p_history.append(loss_p)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            if(i % 10 == 0):\n",
    "                print('Pre-Training at Epoch ' + str(pre_epo) + ' iteration ' + str(i) + ', total loss is '\n",
    "                      + '%.3f' % (loss.cpu().detach().numpy()) + ', proj loss is ' + '%.3f' % (loss_p.cpu().detach().numpy()) \n",
    "                      + ', recon loss is ' + '%.3f' % (loss_r.cpu().detach().numpy()))\n",
    "\n",
    "            if loss_r < thr:\n",
    "                # smaller than certain reconstruction error, -> go to training step\n",
    "                break\n",
    "        \n",
    "        #     if i == int(len_unsup/4):\n",
    "        #         torch.save(model_nn, 'model_pretrain_checkpoint_1.pt')\n",
    "        #     if i == int(len_unsup/2):\n",
    "        #         torch.save(model_nn, 'model_pretrain_checkpoint_1.pt')\n",
    "        # torch.save(model_nn, 'model_nn_pretrain.pt')\n",
    "            \n",
    "    print('--- Go for Training ---')\n",
    "    \n",
    "    for tr_epo in range(train_epoch):\n",
    "        for i, (v_D, label) in enumerate(training_generator_sup):\n",
    "            \n",
    "            tic = time.perf_counter()\n",
    "\n",
    "            v_D = v_D.float().cuda()\n",
    "            recon, code, score, Z_f, z_D = model_nn(v_D)\n",
    "            \n",
    "            label = Variable(torch.from_numpy(np.array(label)).long())\n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            m = torch.nn.Sigmoid()\n",
    "            n = torch.squeeze(m(score))\n",
    "            \n",
    "            loss_c = loss_fct(n, label.float().cuda())\n",
    "            loss_r = recon_loss_coeff * F.binary_cross_entropy(recon, v_D.float())\n",
    "            \n",
    "            loss_p = proj_coeff * (torch.norm(z_D - torch.matmul(code, Z_f)) + lambda1 * torch.sum(torch.abs(code)) / BATCH_SIZE + lambda2 * torch.norm(Z_f, p='fro') / BATCH_SIZE)\n",
    "            \n",
    "            loss = loss_c + loss_r + loss_p\n",
    "            loss_r_history.append(loss_r)\n",
    "            loss_p_history.append(loss_p)\n",
    "            loss_c_history.append(loss_c)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            toc = time.perf_counter()\n",
    "            print(f\"Ran Iteration in {toc - tic:0.4f} seconds\")\n",
    "                    \n",
    "            if(i % 1 == 0):\n",
    "                print('Training at Epoch ' + str(tr_epo) + ' iteration ' + str(i) + ', total loss is ' + '%.3f' % (loss.cpu().detach().numpy()) + ', proj loss is ' + '%.3f' %(loss_p.cpu().detach().numpy()) + ', recon loss is ' + '%.3f' %(loss_r.cpu().detach().numpy()) + ', classification loss is ' + '%.3f' % (loss_c.cpu().detach().numpy()))\n",
    "            \n",
    "        with torch.set_grad_enabled(False):\n",
    "            auc, logits = test_dde_nn(validation_generator_sup, model_nn)\n",
    "            if auc > max_auc:\n",
    "                model_max = copy.deepcopy(model_nn)\n",
    "                max_auc = auc\n",
    "                path = 'model_train_checkpoint_SNAP_EarlyStopping_SemiSup_Full_Run3.pt'\n",
    "                torch.save(model_nn, path)    \n",
    "            print('Test at Epoch '+ str(tr_epo) + ' , AUC: '+ str(auc))\n",
    "        \n",
    "    return model_max, loss_c_history, loss_r_history, loss_p_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Let's use 2 GPUs!\n",
      "--- Data Preparation ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.parallel.data_parallel.DataParallel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'dde_torch.dde_NN_Large_Predictor' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/tmp/ipykernel_8339/3429995564.py:59: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_unsup = pd.read_csv('data/unsup_dataset.csv', names = ['idx', 'input1_SMILES', 'input2_SMILES', 'type']).drop(0)# pairs dataframe input1_smiles, input2_smiles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "--- Pre-training Starts ---\n",
      "--- Go for Training ---\n",
      "Ran Iteration in 14.8509 seconds\n",
      "Training at Epoch 0 iteration 0, total loss is 0.716, proj loss is 0.010, recon loss is 0.005, classification loss is 0.701\n",
      "Ran Iteration in 12.7918 seconds\n",
      "Training at Epoch 0 iteration 1, total loss is 0.849, proj loss is 0.170, recon loss is 0.005, classification loss is 0.673\n",
      "Ran Iteration in 12.6085 seconds\n",
      "Training at Epoch 0 iteration 2, total loss is 0.867, proj loss is 0.237, recon loss is 0.005, classification loss is 0.626\n",
      "Ran Iteration in 12.6564 seconds\n",
      "Training at Epoch 0 iteration 3, total loss is 0.883, proj loss is 0.260, recon loss is 0.005, classification loss is 0.617\n",
      "Ran Iteration in 12.7596 seconds\n",
      "Training at Epoch 0 iteration 4, total loss is 0.857, proj loss is 0.266, recon loss is 0.005, classification loss is 0.586\n",
      "Ran Iteration in 12.7842 seconds\n",
      "Training at Epoch 0 iteration 5, total loss is 0.777, proj loss is 0.253, recon loss is 0.005, classification loss is 0.519\n",
      "Ran Iteration in 12.7902 seconds\n",
      "Training at Epoch 0 iteration 6, total loss is 0.774, proj loss is 0.238, recon loss is 0.005, classification loss is 0.531\n",
      "Ran Iteration in 12.6317 seconds\n",
      "Training at Epoch 0 iteration 7, total loss is 0.774, proj loss is 0.218, recon loss is 0.005, classification loss is 0.551\n",
      "Ran Iteration in 12.7941 seconds\n",
      "Training at Epoch 0 iteration 8, total loss is 0.758, proj loss is 0.198, recon loss is 0.005, classification loss is 0.555\n",
      "Ran Iteration in 12.7753 seconds\n",
      "Training at Epoch 0 iteration 9, total loss is 0.791, proj loss is 0.175, recon loss is 0.005, classification loss is 0.610\n",
      "Ran Iteration in 12.6310 seconds\n",
      "Training at Epoch 0 iteration 10, total loss is 0.675, proj loss is 0.154, recon loss is 0.005, classification loss is 0.516\n",
      "Ran Iteration in 12.6226 seconds\n",
      "Training at Epoch 0 iteration 11, total loss is 0.602, proj loss is 0.133, recon loss is 0.005, classification loss is 0.464\n",
      "Ran Iteration in 12.7768 seconds\n",
      "Training at Epoch 0 iteration 12, total loss is 0.579, proj loss is 0.111, recon loss is 0.005, classification loss is 0.463\n",
      "Ran Iteration in 12.6528 seconds\n",
      "Training at Epoch 0 iteration 13, total loss is 0.569, proj loss is 0.095, recon loss is 0.005, classification loss is 0.469\n",
      "Ran Iteration in 12.7946 seconds\n",
      "Training at Epoch 0 iteration 14, total loss is 0.616, proj loss is 0.082, recon loss is 0.005, classification loss is 0.529\n",
      "Ran Iteration in 12.7776 seconds\n",
      "Training at Epoch 0 iteration 15, total loss is 0.536, proj loss is 0.071, recon loss is 0.005, classification loss is 0.460\n",
      "Ran Iteration in 12.7959 seconds\n",
      "Training at Epoch 0 iteration 16, total loss is 0.494, proj loss is 0.067, recon loss is 0.005, classification loss is 0.422\n",
      "Ran Iteration in 12.8158 seconds\n",
      "Training at Epoch 0 iteration 17, total loss is 0.537, proj loss is 0.070, recon loss is 0.005, classification loss is 0.462\n",
      "Ran Iteration in 12.7566 seconds\n",
      "Training at Epoch 0 iteration 18, total loss is 0.499, proj loss is 0.080, recon loss is 0.005, classification loss is 0.414\n",
      "Ran Iteration in 12.7708 seconds\n",
      "Training at Epoch 0 iteration 19, total loss is 0.586, proj loss is 0.089, recon loss is 0.005, classification loss is 0.492\n",
      "Ran Iteration in 12.7719 seconds\n",
      "Training at Epoch 0 iteration 20, total loss is 0.602, proj loss is 0.094, recon loss is 0.005, classification loss is 0.503\n",
      "Ran Iteration in 12.6810 seconds\n",
      "Training at Epoch 0 iteration 21, total loss is 0.472, proj loss is 0.090, recon loss is 0.005, classification loss is 0.377\n",
      "Ran Iteration in 12.7809 seconds\n",
      "Training at Epoch 0 iteration 22, total loss is 0.505, proj loss is 0.081, recon loss is 0.005, classification loss is 0.419\n",
      "Ran Iteration in 12.6197 seconds\n",
      "Training at Epoch 0 iteration 23, total loss is 0.489, proj loss is 0.069, recon loss is 0.005, classification loss is 0.415\n",
      "Ran Iteration in 12.7782 seconds\n",
      "Training at Epoch 0 iteration 24, total loss is 0.476, proj loss is 0.061, recon loss is 0.005, classification loss is 0.410\n",
      "Ran Iteration in 12.8056 seconds\n",
      "Training at Epoch 0 iteration 25, total loss is 0.495, proj loss is 0.063, recon loss is 0.005, classification loss is 0.427\n",
      "Ran Iteration in 12.8111 seconds\n",
      "Training at Epoch 0 iteration 26, total loss is 0.435, proj loss is 0.068, recon loss is 0.005, classification loss is 0.362\n",
      "Ran Iteration in 12.7987 seconds\n",
      "Training at Epoch 0 iteration 27, total loss is 0.439, proj loss is 0.077, recon loss is 0.005, classification loss is 0.357\n",
      "Ran Iteration in 12.7784 seconds\n",
      "Training at Epoch 0 iteration 28, total loss is 0.448, proj loss is 0.080, recon loss is 0.005, classification loss is 0.364\n",
      "Ran Iteration in 12.7648 seconds\n",
      "Training at Epoch 0 iteration 29, total loss is 0.464, proj loss is 0.077, recon loss is 0.005, classification loss is 0.383\n",
      "Ran Iteration in 12.8017 seconds\n",
      "Training at Epoch 0 iteration 30, total loss is 0.466, proj loss is 0.065, recon loss is 0.005, classification loss is 0.396\n",
      "Ran Iteration in 12.7738 seconds\n",
      "Training at Epoch 0 iteration 31, total loss is 0.401, proj loss is 0.053, recon loss is 0.005, classification loss is 0.343\n",
      "Ran Iteration in 12.7833 seconds\n",
      "Training at Epoch 0 iteration 32, total loss is 0.420, proj loss is 0.065, recon loss is 0.005, classification loss is 0.350\n",
      "Ran Iteration in 12.7148 seconds\n",
      "Training at Epoch 0 iteration 33, total loss is 0.484, proj loss is 0.081, recon loss is 0.005, classification loss is 0.398\n",
      "Ran Iteration in 12.7810 seconds\n",
      "Training at Epoch 0 iteration 34, total loss is 0.464, proj loss is 0.089, recon loss is 0.005, classification loss is 0.370\n",
      "Ran Iteration in 12.6363 seconds\n",
      "Training at Epoch 0 iteration 35, total loss is 0.501, proj loss is 0.089, recon loss is 0.005, classification loss is 0.407\n",
      "Ran Iteration in 12.7890 seconds\n",
      "Training at Epoch 0 iteration 36, total loss is 0.439, proj loss is 0.080, recon loss is 0.005, classification loss is 0.353\n",
      "Ran Iteration in 12.6147 seconds\n",
      "Training at Epoch 0 iteration 37, total loss is 0.420, proj loss is 0.068, recon loss is 0.005, classification loss is 0.347\n",
      "Ran Iteration in 12.8079 seconds\n",
      "Training at Epoch 0 iteration 38, total loss is 0.427, proj loss is 0.062, recon loss is 0.005, classification loss is 0.361\n",
      "Ran Iteration in 12.8131 seconds\n",
      "Training at Epoch 0 iteration 39, total loss is 0.393, proj loss is 0.068, recon loss is 0.005, classification loss is 0.320\n",
      "Ran Iteration in 12.7726 seconds\n",
      "Training at Epoch 0 iteration 40, total loss is 0.391, proj loss is 0.073, recon loss is 0.005, classification loss is 0.314\n",
      "Ran Iteration in 12.6591 seconds\n",
      "Training at Epoch 0 iteration 41, total loss is 0.436, proj loss is 0.076, recon loss is 0.005, classification loss is 0.355\n",
      "Ran Iteration in 12.7690 seconds\n",
      "Training at Epoch 0 iteration 42, total loss is 0.447, proj loss is 0.078, recon loss is 0.005, classification loss is 0.364\n",
      "Ran Iteration in 12.8000 seconds\n",
      "Training at Epoch 0 iteration 43, total loss is 0.392, proj loss is 0.073, recon loss is 0.005, classification loss is 0.313\n",
      "Ran Iteration in 12.7782 seconds\n",
      "Training at Epoch 0 iteration 44, total loss is 0.381, proj loss is 0.069, recon loss is 0.005, classification loss is 0.308\n",
      "Ran Iteration in 12.7966 seconds\n",
      "Training at Epoch 0 iteration 45, total loss is 0.363, proj loss is 0.067, recon loss is 0.005, classification loss is 0.291\n",
      "Ran Iteration in 12.7622 seconds\n",
      "Training at Epoch 0 iteration 46, total loss is 0.304, proj loss is 0.069, recon loss is 0.005, classification loss is 0.229\n",
      "Ran Iteration in 12.8377 seconds\n",
      "Training at Epoch 0 iteration 47, total loss is 0.332, proj loss is 0.070, recon loss is 0.005, classification loss is 0.257\n",
      "Ran Iteration in 12.7104 seconds\n",
      "Training at Epoch 0 iteration 48, total loss is 0.436, proj loss is 0.070, recon loss is 0.005, classification loss is 0.361\n",
      "Ran Iteration in 12.7872 seconds\n",
      "Training at Epoch 0 iteration 49, total loss is 0.362, proj loss is 0.070, recon loss is 0.005, classification loss is 0.287\n",
      "Ran Iteration in 12.6647 seconds\n",
      "Training at Epoch 0 iteration 50, total loss is 0.456, proj loss is 0.071, recon loss is 0.005, classification loss is 0.380\n",
      "Ran Iteration in 12.7819 seconds\n",
      "Training at Epoch 0 iteration 51, total loss is 0.379, proj loss is 0.080, recon loss is 0.005, classification loss is 0.294\n",
      "Ran Iteration in 12.7707 seconds\n",
      "Training at Epoch 0 iteration 52, total loss is 0.371, proj loss is 0.085, recon loss is 0.005, classification loss is 0.282\n",
      "Ran Iteration in 12.7861 seconds\n",
      "Training at Epoch 0 iteration 53, total loss is 0.339, proj loss is 0.082, recon loss is 0.005, classification loss is 0.252\n",
      "Ran Iteration in 12.7540 seconds\n",
      "Training at Epoch 0 iteration 54, total loss is 0.349, proj loss is 0.074, recon loss is 0.005, classification loss is 0.271\n",
      "Ran Iteration in 12.8626 seconds\n",
      "Training at Epoch 0 iteration 55, total loss is 0.343, proj loss is 0.062, recon loss is 0.005, classification loss is 0.276\n",
      "Ran Iteration in 12.7606 seconds\n",
      "Training at Epoch 0 iteration 56, total loss is 0.322, proj loss is 0.067, recon loss is 0.005, classification loss is 0.250\n",
      "Ran Iteration in 12.7803 seconds\n",
      "Training at Epoch 0 iteration 57, total loss is 0.402, proj loss is 0.072, recon loss is 0.005, classification loss is 0.325\n",
      "Ran Iteration in 12.6223 seconds\n",
      "Training at Epoch 0 iteration 58, total loss is 0.349, proj loss is 0.079, recon loss is 0.005, classification loss is 0.265\n",
      "Ran Iteration in 12.8013 seconds\n",
      "Training at Epoch 0 iteration 59, total loss is 0.358, proj loss is 0.081, recon loss is 0.005, classification loss is 0.272\n",
      "Ran Iteration in 12.6359 seconds\n",
      "Training at Epoch 0 iteration 60, total loss is 0.346, proj loss is 0.076, recon loss is 0.005, classification loss is 0.264\n",
      "Ran Iteration in 12.7964 seconds\n",
      "Training at Epoch 0 iteration 61, total loss is 0.392, proj loss is 0.070, recon loss is 0.005, classification loss is 0.317\n",
      "Ran Iteration in 12.7753 seconds\n",
      "Training at Epoch 0 iteration 62, total loss is 0.428, proj loss is 0.062, recon loss is 0.005, classification loss is 0.361\n",
      "Ran Iteration in 12.8336 seconds\n",
      "Training at Epoch 0 iteration 63, total loss is 0.424, proj loss is 0.053, recon loss is 0.005, classification loss is 0.365\n",
      "Ran Iteration in 12.7834 seconds\n",
      "Training at Epoch 0 iteration 64, total loss is 0.341, proj loss is 0.057, recon loss is 0.005, classification loss is 0.279\n",
      "Ran Iteration in 12.7781 seconds\n",
      "Training at Epoch 0 iteration 65, total loss is 0.395, proj loss is 0.070, recon loss is 0.005, classification loss is 0.319\n",
      "Ran Iteration in 12.7990 seconds\n",
      "Training at Epoch 0 iteration 66, total loss is 0.320, proj loss is 0.086, recon loss is 0.005, classification loss is 0.229\n",
      "Ran Iteration in 12.7753 seconds\n",
      "Training at Epoch 0 iteration 67, total loss is 0.390, proj loss is 0.092, recon loss is 0.005, classification loss is 0.293\n",
      "Ran Iteration in 12.8097 seconds\n",
      "Training at Epoch 0 iteration 68, total loss is 0.354, proj loss is 0.090, recon loss is 0.005, classification loss is 0.259\n",
      "Ran Iteration in 12.6592 seconds\n",
      "Training at Epoch 0 iteration 69, total loss is 0.339, proj loss is 0.081, recon loss is 0.005, classification loss is 0.253\n",
      "Ran Iteration in 12.8022 seconds\n",
      "Training at Epoch 0 iteration 70, total loss is 0.427, proj loss is 0.066, recon loss is 0.005, classification loss is 0.355\n",
      "Ran Iteration in 12.8230 seconds\n",
      "Training at Epoch 0 iteration 71, total loss is 0.368, proj loss is 0.053, recon loss is 0.005, classification loss is 0.311\n",
      "Ran Iteration in 12.8146 seconds\n",
      "Training at Epoch 0 iteration 72, total loss is 0.372, proj loss is 0.050, recon loss is 0.005, classification loss is 0.317\n",
      "Ran Iteration in 12.8002 seconds\n",
      "Training at Epoch 0 iteration 73, total loss is 0.382, proj loss is 0.067, recon loss is 0.005, classification loss is 0.309\n",
      "Ran Iteration in 12.7978 seconds\n",
      "Training at Epoch 0 iteration 74, total loss is 0.397, proj loss is 0.087, recon loss is 0.005, classification loss is 0.305\n",
      "Ran Iteration in 12.8043 seconds\n",
      "Training at Epoch 0 iteration 75, total loss is 0.376, proj loss is 0.103, recon loss is 0.005, classification loss is 0.268\n",
      "Ran Iteration in 12.7682 seconds\n",
      "Training at Epoch 0 iteration 76, total loss is 0.351, proj loss is 0.110, recon loss is 0.005, classification loss is 0.236\n",
      "Ran Iteration in 12.6341 seconds\n",
      "Training at Epoch 0 iteration 77, total loss is 0.433, proj loss is 0.108, recon loss is 0.005, classification loss is 0.320\n",
      "Ran Iteration in 12.7906 seconds\n",
      "Training at Epoch 0 iteration 78, total loss is 0.425, proj loss is 0.098, recon loss is 0.005, classification loss is 0.322\n",
      "Ran Iteration in 12.8027 seconds\n",
      "Training at Epoch 0 iteration 79, total loss is 0.412, proj loss is 0.083, recon loss is 0.005, classification loss is 0.324\n",
      "Ran Iteration in 12.7832 seconds\n",
      "Training at Epoch 0 iteration 80, total loss is 0.329, proj loss is 0.065, recon loss is 0.005, classification loss is 0.259\n",
      "Ran Iteration in 12.7978 seconds\n",
      "Training at Epoch 0 iteration 81, total loss is 0.325, proj loss is 0.058, recon loss is 0.005, classification loss is 0.262\n",
      "Ran Iteration in 12.6475 seconds\n",
      "Training at Epoch 0 iteration 82, total loss is 0.301, proj loss is 0.064, recon loss is 0.005, classification loss is 0.232\n",
      "Ran Iteration in 12.8076 seconds\n",
      "Training at Epoch 0 iteration 83, total loss is 0.357, proj loss is 0.070, recon loss is 0.005, classification loss is 0.282\n",
      "Ran Iteration in 12.7774 seconds\n",
      "Training at Epoch 0 iteration 84, total loss is 0.462, proj loss is 0.076, recon loss is 0.005, classification loss is 0.381\n",
      "Ran Iteration in 12.7966 seconds\n",
      "Training at Epoch 0 iteration 85, total loss is 0.371, proj loss is 0.085, recon loss is 0.005, classification loss is 0.280\n",
      "Ran Iteration in 12.7796 seconds\n",
      "Training at Epoch 0 iteration 86, total loss is 0.320, proj loss is 0.091, recon loss is 0.005, classification loss is 0.224\n",
      "Ran Iteration in 12.7910 seconds\n",
      "Training at Epoch 0 iteration 87, total loss is 0.339, proj loss is 0.092, recon loss is 0.005, classification loss is 0.242\n",
      "Ran Iteration in 12.7601 seconds\n",
      "Training at Epoch 0 iteration 88, total loss is 0.379, proj loss is 0.086, recon loss is 0.005, classification loss is 0.287\n",
      "Ran Iteration in 12.8627 seconds\n",
      "Training at Epoch 0 iteration 89, total loss is 0.402, proj loss is 0.074, recon loss is 0.005, classification loss is 0.323\n",
      "Ran Iteration in 12.7992 seconds\n",
      "Training at Epoch 0 iteration 90, total loss is 0.291, proj loss is 0.059, recon loss is 0.005, classification loss is 0.227\n",
      "Ran Iteration in 12.7646 seconds\n",
      "Training at Epoch 0 iteration 91, total loss is 0.346, proj loss is 0.051, recon loss is 0.005, classification loss is 0.291\n",
      "Ran Iteration in 12.8389 seconds\n",
      "Training at Epoch 0 iteration 92, total loss is 0.403, proj loss is 0.060, recon loss is 0.005, classification loss is 0.339\n",
      "Ran Iteration in 12.8046 seconds\n",
      "Training at Epoch 0 iteration 93, total loss is 0.375, proj loss is 0.074, recon loss is 0.005, classification loss is 0.296\n",
      "Ran Iteration in 12.7612 seconds\n",
      "Training at Epoch 0 iteration 94, total loss is 0.375, proj loss is 0.084, recon loss is 0.005, classification loss is 0.286\n",
      "Ran Iteration in 12.7930 seconds\n",
      "Training at Epoch 0 iteration 95, total loss is 0.382, proj loss is 0.083, recon loss is 0.005, classification loss is 0.294\n",
      "Ran Iteration in 12.6109 seconds\n",
      "Training at Epoch 0 iteration 96, total loss is 0.275, proj loss is 0.076, recon loss is 0.005, classification loss is 0.194\n",
      "Ran Iteration in 12.7723 seconds\n",
      "Training at Epoch 0 iteration 97, total loss is 0.301, proj loss is 0.065, recon loss is 0.005, classification loss is 0.231\n",
      "Ran Iteration in 12.7853 seconds\n",
      "Training at Epoch 0 iteration 98, total loss is 0.318, proj loss is 0.053, recon loss is 0.005, classification loss is 0.260\n",
      "Ran Iteration in 12.7873 seconds\n",
      "Training at Epoch 0 iteration 99, total loss is 0.371, proj loss is 0.055, recon loss is 0.005, classification loss is 0.311\n",
      "Ran Iteration in 12.8088 seconds\n",
      "Training at Epoch 0 iteration 100, total loss is 0.318, proj loss is 0.074, recon loss is 0.005, classification loss is 0.239\n",
      "Ran Iteration in 12.8073 seconds\n",
      "Training at Epoch 0 iteration 101, total loss is 0.432, proj loss is 0.085, recon loss is 0.005, classification loss is 0.342\n",
      "Ran Iteration in 12.7730 seconds\n",
      "Training at Epoch 0 iteration 102, total loss is 0.382, proj loss is 0.090, recon loss is 0.005, classification loss is 0.287\n",
      "Ran Iteration in 12.7895 seconds\n",
      "Training at Epoch 0 iteration 103, total loss is 0.336, proj loss is 0.088, recon loss is 0.005, classification loss is 0.243\n",
      "Ran Iteration in 12.7752 seconds\n",
      "Training at Epoch 0 iteration 104, total loss is 0.330, proj loss is 0.079, recon loss is 0.005, classification loss is 0.246\n",
      "Ran Iteration in 12.8156 seconds\n",
      "Training at Epoch 0 iteration 105, total loss is 0.357, proj loss is 0.067, recon loss is 0.005, classification loss is 0.285\n",
      "Ran Iteration in 12.7342 seconds\n",
      "Training at Epoch 0 iteration 106, total loss is 0.324, proj loss is 0.056, recon loss is 0.005, classification loss is 0.263\n",
      "Ran Iteration in 12.6266 seconds\n",
      "Training at Epoch 0 iteration 107, total loss is 0.336, proj loss is 0.054, recon loss is 0.005, classification loss is 0.277\n",
      "Ran Iteration in 12.6336 seconds\n",
      "Training at Epoch 0 iteration 108, total loss is 0.301, proj loss is 0.067, recon loss is 0.005, classification loss is 0.229\n",
      "Ran Iteration in 12.8571 seconds\n",
      "Training at Epoch 0 iteration 109, total loss is 0.305, proj loss is 0.077, recon loss is 0.005, classification loss is 0.223\n",
      "Ran Iteration in 12.7742 seconds\n",
      "Training at Epoch 0 iteration 110, total loss is 0.305, proj loss is 0.079, recon loss is 0.005, classification loss is 0.220\n",
      "Ran Iteration in 12.8014 seconds\n",
      "Training at Epoch 0 iteration 111, total loss is 0.298, proj loss is 0.074, recon loss is 0.005, classification loss is 0.218\n",
      "Ran Iteration in 12.6253 seconds\n",
      "Training at Epoch 0 iteration 112, total loss is 0.329, proj loss is 0.066, recon loss is 0.005, classification loss is 0.258\n",
      "Ran Iteration in 12.7981 seconds\n",
      "Training at Epoch 0 iteration 113, total loss is 0.274, proj loss is 0.059, recon loss is 0.005, classification loss is 0.210\n",
      "Ran Iteration in 12.7644 seconds\n",
      "Training at Epoch 0 iteration 114, total loss is 0.281, proj loss is 0.052, recon loss is 0.005, classification loss is 0.225\n",
      "Ran Iteration in 12.7813 seconds\n",
      "Training at Epoch 0 iteration 115, total loss is 0.302, proj loss is 0.052, recon loss is 0.005, classification loss is 0.245\n",
      "Ran Iteration in 12.8259 seconds\n",
      "Training at Epoch 0 iteration 116, total loss is 0.369, proj loss is 0.066, recon loss is 0.005, classification loss is 0.298\n",
      "Ran Iteration in 12.8013 seconds\n",
      "Training at Epoch 0 iteration 117, total loss is 0.371, proj loss is 0.081, recon loss is 0.005, classification loss is 0.285\n",
      "Ran Iteration in 12.6483 seconds\n",
      "Training at Epoch 0 iteration 118, total loss is 0.368, proj loss is 0.091, recon loss is 0.005, classification loss is 0.272\n",
      "Ran Iteration in 12.7058 seconds\n",
      "Training at Epoch 0 iteration 119, total loss is 0.328, proj loss is 0.092, recon loss is 0.005, classification loss is 0.231\n",
      "Ran Iteration in 12.7957 seconds\n",
      "Training at Epoch 0 iteration 120, total loss is 0.289, proj loss is 0.085, recon loss is 0.005, classification loss is 0.199\n",
      "Ran Iteration in 12.8028 seconds\n",
      "Training at Epoch 0 iteration 121, total loss is 0.285, proj loss is 0.074, recon loss is 0.005, classification loss is 0.206\n",
      "Ran Iteration in 12.8291 seconds\n",
      "Training at Epoch 0 iteration 122, total loss is 0.278, proj loss is 0.062, recon loss is 0.005, classification loss is 0.211\n",
      "Ran Iteration in 12.8087 seconds\n",
      "Training at Epoch 0 iteration 123, total loss is 0.293, proj loss is 0.053, recon loss is 0.005, classification loss is 0.235\n",
      "Ran Iteration in 12.6408 seconds\n",
      "Training at Epoch 0 iteration 124, total loss is 0.268, proj loss is 0.052, recon loss is 0.005, classification loss is 0.211\n",
      "Ran Iteration in 12.8599 seconds\n",
      "Training at Epoch 0 iteration 125, total loss is 0.236, proj loss is 0.058, recon loss is 0.005, classification loss is 0.172\n",
      "Ran Iteration in 12.7987 seconds\n",
      "Training at Epoch 0 iteration 126, total loss is 0.293, proj loss is 0.067, recon loss is 0.005, classification loss is 0.221\n",
      "Ran Iteration in 12.7865 seconds\n",
      "Training at Epoch 0 iteration 127, total loss is 0.253, proj loss is 0.071, recon loss is 0.005, classification loss is 0.177\n",
      "Ran Iteration in 12.6263 seconds\n",
      "Training at Epoch 0 iteration 128, total loss is 0.369, proj loss is 0.070, recon loss is 0.005, classification loss is 0.294\n",
      "Ran Iteration in 12.8982 seconds\n",
      "Training at Epoch 0 iteration 129, total loss is 0.263, proj loss is 0.061, recon loss is 0.005, classification loss is 0.197\n",
      "Ran Iteration in 12.7821 seconds\n",
      "Training at Epoch 0 iteration 130, total loss is 0.294, proj loss is 0.057, recon loss is 0.005, classification loss is 0.232\n",
      "Ran Iteration in 12.8010 seconds\n",
      "Training at Epoch 0 iteration 131, total loss is 0.340, proj loss is 0.066, recon loss is 0.005, classification loss is 0.269\n",
      "Ran Iteration in 12.7779 seconds\n",
      "Training at Epoch 0 iteration 132, total loss is 0.375, proj loss is 0.078, recon loss is 0.005, classification loss is 0.291\n",
      "Ran Iteration in 12.8031 seconds\n",
      "Training at Epoch 0 iteration 133, total loss is 0.298, proj loss is 0.088, recon loss is 0.005, classification loss is 0.205\n",
      "Ran Iteration in 12.8165 seconds\n",
      "Training at Epoch 0 iteration 134, total loss is 0.375, proj loss is 0.091, recon loss is 0.005, classification loss is 0.278\n",
      "Ran Iteration in 12.7691 seconds\n",
      "Training at Epoch 0 iteration 135, total loss is 0.284, proj loss is 0.092, recon loss is 0.005, classification loss is 0.187\n",
      "Ran Iteration in 12.8443 seconds\n",
      "Training at Epoch 0 iteration 136, total loss is 0.345, proj loss is 0.086, recon loss is 0.005, classification loss is 0.253\n",
      "Ran Iteration in 12.8135 seconds\n",
      "Training at Epoch 0 iteration 137, total loss is 0.333, proj loss is 0.074, recon loss is 0.005, classification loss is 0.254\n",
      "Ran Iteration in 12.7781 seconds\n",
      "Training at Epoch 0 iteration 138, total loss is 0.266, proj loss is 0.061, recon loss is 0.005, classification loss is 0.200\n",
      "Ran Iteration in 12.7876 seconds\n",
      "Training at Epoch 0 iteration 139, total loss is 0.373, proj loss is 0.055, recon loss is 0.005, classification loss is 0.314\n",
      "Ran Iteration in 12.6345 seconds\n",
      "Training at Epoch 0 iteration 140, total loss is 0.332, proj loss is 0.065, recon loss is 0.005, classification loss is 0.262\n",
      "Ran Iteration in 12.7763 seconds\n",
      "Training at Epoch 0 iteration 141, total loss is 0.263, proj loss is 0.081, recon loss is 0.005, classification loss is 0.177\n",
      "Ran Iteration in 12.7909 seconds\n",
      "Training at Epoch 0 iteration 142, total loss is 0.357, proj loss is 0.095, recon loss is 0.005, classification loss is 0.257\n",
      "Ran Iteration in 12.8104 seconds\n",
      "Training at Epoch 0 iteration 143, total loss is 0.324, proj loss is 0.091, recon loss is 0.005, classification loss is 0.228\n",
      "Ran Iteration in 12.6642 seconds\n",
      "Training at Epoch 0 iteration 144, total loss is 0.292, proj loss is 0.083, recon loss is 0.005, classification loss is 0.204\n",
      "Ran Iteration in 12.6281 seconds\n",
      "Training at Epoch 0 iteration 145, total loss is 0.269, proj loss is 0.069, recon loss is 0.005, classification loss is 0.195\n",
      "Ran Iteration in 12.7930 seconds\n",
      "Training at Epoch 0 iteration 146, total loss is 0.293, proj loss is 0.056, recon loss is 0.005, classification loss is 0.231\n",
      "Ran Iteration in 12.7941 seconds\n",
      "Training at Epoch 0 iteration 147, total loss is 0.336, proj loss is 0.051, recon loss is 0.005, classification loss is 0.280\n",
      "Ran Iteration in 12.7789 seconds\n",
      "Training at Epoch 0 iteration 148, total loss is 0.277, proj loss is 0.067, recon loss is 0.005, classification loss is 0.206\n",
      "Ran Iteration in 12.7731 seconds\n",
      "Training at Epoch 0 iteration 149, total loss is 0.323, proj loss is 0.088, recon loss is 0.005, classification loss is 0.231\n",
      "Ran Iteration in 12.7964 seconds\n",
      "Training at Epoch 0 iteration 150, total loss is 0.357, proj loss is 0.103, recon loss is 0.005, classification loss is 0.249\n",
      "Ran Iteration in 12.7541 seconds\n",
      "Training at Epoch 0 iteration 151, total loss is 0.365, proj loss is 0.110, recon loss is 0.005, classification loss is 0.250\n",
      "Ran Iteration in 12.8083 seconds\n",
      "Training at Epoch 0 iteration 152, total loss is 0.385, proj loss is 0.105, recon loss is 0.005, classification loss is 0.275\n",
      "Ran Iteration in 12.8855 seconds\n",
      "Training at Epoch 0 iteration 153, total loss is 0.369, proj loss is 0.092, recon loss is 0.005, classification loss is 0.272\n",
      "Ran Iteration in 12.7824 seconds\n",
      "Training at Epoch 0 iteration 154, total loss is 0.253, proj loss is 0.073, recon loss is 0.005, classification loss is 0.175\n",
      "Ran Iteration in 12.6474 seconds\n",
      "Training at Epoch 0 iteration 155, total loss is 0.268, proj loss is 0.052, recon loss is 0.005, classification loss is 0.211\n",
      "Ran Iteration in 12.8306 seconds\n",
      "Training at Epoch 0 iteration 156, total loss is 0.331, proj loss is 0.048, recon loss is 0.005, classification loss is 0.278\n",
      "Ran Iteration in 12.7926 seconds\n",
      "Training at Epoch 0 iteration 157, total loss is 0.318, proj loss is 0.072, recon loss is 0.005, classification loss is 0.241\n",
      "Ran Iteration in 12.7931 seconds\n",
      "Training at Epoch 0 iteration 158, total loss is 0.286, proj loss is 0.092, recon loss is 0.005, classification loss is 0.189\n",
      "Ran Iteration in 12.6608 seconds\n",
      "Training at Epoch 0 iteration 159, total loss is 0.328, proj loss is 0.101, recon loss is 0.005, classification loss is 0.222\n",
      "Ran Iteration in 12.8410 seconds\n",
      "Training at Epoch 0 iteration 160, total loss is 0.322, proj loss is 0.098, recon loss is 0.005, classification loss is 0.219\n",
      "Ran Iteration in 12.7758 seconds\n",
      "Training at Epoch 0 iteration 161, total loss is 0.342, proj loss is 0.086, recon loss is 0.005, classification loss is 0.251\n",
      "Ran Iteration in 12.7799 seconds\n",
      "Training at Epoch 0 iteration 162, total loss is 0.283, proj loss is 0.066, recon loss is 0.005, classification loss is 0.212\n",
      "Ran Iteration in 12.8095 seconds\n",
      "Training at Epoch 0 iteration 163, total loss is 0.256, proj loss is 0.046, recon loss is 0.005, classification loss is 0.205\n",
      "Ran Iteration in 12.7796 seconds\n",
      "Training at Epoch 0 iteration 164, total loss is 0.290, proj loss is 0.052, recon loss is 0.005, classification loss is 0.234\n",
      "Ran Iteration in 12.6485 seconds\n",
      "Training at Epoch 0 iteration 165, total loss is 0.304, proj loss is 0.068, recon loss is 0.005, classification loss is 0.231\n",
      "Ran Iteration in 12.8192 seconds\n",
      "Training at Epoch 0 iteration 166, total loss is 0.306, proj loss is 0.087, recon loss is 0.005, classification loss is 0.215\n",
      "Ran Iteration in 12.7735 seconds\n",
      "Training at Epoch 0 iteration 167, total loss is 0.293, proj loss is 0.099, recon loss is 0.005, classification loss is 0.189\n",
      "Ran Iteration in 12.6600 seconds\n",
      "Training at Epoch 0 iteration 168, total loss is 0.344, proj loss is 0.103, recon loss is 0.005, classification loss is 0.236\n",
      "Ran Iteration in 12.6266 seconds\n",
      "Training at Epoch 0 iteration 169, total loss is 0.296, proj loss is 0.097, recon loss is 0.005, classification loss is 0.194\n",
      "Ran Iteration in 12.6373 seconds\n",
      "Training at Epoch 0 iteration 170, total loss is 0.286, proj loss is 0.083, recon loss is 0.005, classification loss is 0.198\n",
      "Ran Iteration in 12.7643 seconds\n",
      "Training at Epoch 0 iteration 171, total loss is 0.244, proj loss is 0.063, recon loss is 0.005, classification loss is 0.176\n",
      "Ran Iteration in 12.7987 seconds\n",
      "Training at Epoch 0 iteration 172, total loss is 0.256, proj loss is 0.044, recon loss is 0.005, classification loss is 0.207\n",
      "Ran Iteration in 12.6501 seconds\n",
      "Training at Epoch 0 iteration 173, total loss is 0.319, proj loss is 0.045, recon loss is 0.005, classification loss is 0.269\n",
      "Ran Iteration in 12.7907 seconds\n",
      "Training at Epoch 0 iteration 174, total loss is 0.290, proj loss is 0.076, recon loss is 0.005, classification loss is 0.209\n",
      "Ran Iteration in 12.8100 seconds\n",
      "Training at Epoch 0 iteration 175, total loss is 0.334, proj loss is 0.107, recon loss is 0.005, classification loss is 0.223\n",
      "Ran Iteration in 12.6490 seconds\n",
      "Training at Epoch 0 iteration 176, total loss is 0.353, proj loss is 0.122, recon loss is 0.005, classification loss is 0.227\n",
      "Ran Iteration in 12.6193 seconds\n",
      "Training at Epoch 0 iteration 177, total loss is 0.375, proj loss is 0.127, recon loss is 0.005, classification loss is 0.243\n",
      "Ran Iteration in 12.7319 seconds\n",
      "Training at Epoch 0 iteration 178, total loss is 0.309, proj loss is 0.118, recon loss is 0.005, classification loss is 0.186\n",
      "Ran Iteration in 12.7769 seconds\n",
      "Training at Epoch 0 iteration 179, total loss is 0.282, proj loss is 0.099, recon loss is 0.005, classification loss is 0.178\n",
      "Ran Iteration in 12.7752 seconds\n",
      "Training at Epoch 0 iteration 180, total loss is 0.280, proj loss is 0.075, recon loss is 0.005, classification loss is 0.200\n",
      "Ran Iteration in 12.7879 seconds\n",
      "Training at Epoch 0 iteration 181, total loss is 0.259, proj loss is 0.049, recon loss is 0.005, classification loss is 0.205\n",
      "Ran Iteration in 12.7810 seconds\n",
      "Training at Epoch 0 iteration 182, total loss is 0.221, proj loss is 0.053, recon loss is 0.005, classification loss is 0.164\n",
      "Ran Iteration in 12.7622 seconds\n",
      "Training at Epoch 0 iteration 183, total loss is 0.342, proj loss is 0.077, recon loss is 0.005, classification loss is 0.260\n",
      "Ran Iteration in 12.7930 seconds\n",
      "Training at Epoch 0 iteration 184, total loss is 0.269, proj loss is 0.098, recon loss is 0.005, classification loss is 0.167\n",
      "Ran Iteration in 12.6304 seconds\n",
      "Training at Epoch 0 iteration 185, total loss is 0.304, proj loss is 0.108, recon loss is 0.005, classification loss is 0.191\n",
      "Ran Iteration in 12.8289 seconds\n",
      "Training at Epoch 0 iteration 186, total loss is 0.303, proj loss is 0.107, recon loss is 0.005, classification loss is 0.191\n",
      "Ran Iteration in 12.7501 seconds\n",
      "Training at Epoch 0 iteration 187, total loss is 0.314, proj loss is 0.096, recon loss is 0.005, classification loss is 0.214\n",
      "Ran Iteration in 12.6628 seconds\n",
      "Training at Epoch 0 iteration 188, total loss is 0.299, proj loss is 0.078, recon loss is 0.005, classification loss is 0.216\n",
      "Ran Iteration in 12.8165 seconds\n",
      "Training at Epoch 0 iteration 189, total loss is 0.221, proj loss is 0.054, recon loss is 0.005, classification loss is 0.162\n",
      "Ran Iteration in 12.6396 seconds\n",
      "Training at Epoch 0 iteration 190, total loss is 0.363, proj loss is 0.046, recon loss is 0.005, classification loss is 0.312\n",
      "Ran Iteration in 12.7786 seconds\n",
      "Training at Epoch 0 iteration 191, total loss is 0.283, proj loss is 0.071, recon loss is 0.005, classification loss is 0.207\n",
      "Ran Iteration in 12.7800 seconds\n",
      "Training at Epoch 0 iteration 192, total loss is 0.268, proj loss is 0.094, recon loss is 0.005, classification loss is 0.169\n",
      "Ran Iteration in 12.7754 seconds\n",
      "Training at Epoch 0 iteration 193, total loss is 0.334, proj loss is 0.108, recon loss is 0.005, classification loss is 0.221\n",
      "Ran Iteration in 12.6465 seconds\n",
      "Training at Epoch 0 iteration 194, total loss is 0.301, proj loss is 0.111, recon loss is 0.005, classification loss is 0.186\n",
      "Ran Iteration in 12.8778 seconds\n",
      "Training at Epoch 0 iteration 195, total loss is 0.333, proj loss is 0.105, recon loss is 0.005, classification loss is 0.223\n",
      "Ran Iteration in 12.6625 seconds\n",
      "Training at Epoch 0 iteration 196, total loss is 0.287, proj loss is 0.089, recon loss is 0.005, classification loss is 0.193\n",
      "Ran Iteration in 12.6478 seconds\n",
      "Training at Epoch 0 iteration 197, total loss is 0.296, proj loss is 0.070, recon loss is 0.005, classification loss is 0.221\n",
      "Ran Iteration in 12.8807 seconds\n",
      "Training at Epoch 0 iteration 198, total loss is 0.316, proj loss is 0.055, recon loss is 0.005, classification loss is 0.256\n",
      "Ran Iteration in 12.6297 seconds\n",
      "Training at Epoch 0 iteration 199, total loss is 0.265, proj loss is 0.056, recon loss is 0.005, classification loss is 0.205\n",
      "Ran Iteration in 12.7990 seconds\n",
      "Training at Epoch 0 iteration 200, total loss is 0.294, proj loss is 0.074, recon loss is 0.005, classification loss is 0.215\n",
      "Ran Iteration in 12.6614 seconds\n",
      "Training at Epoch 0 iteration 201, total loss is 0.260, proj loss is 0.089, recon loss is 0.005, classification loss is 0.166\n",
      "Ran Iteration in 12.7947 seconds\n",
      "Training at Epoch 0 iteration 202, total loss is 0.301, proj loss is 0.095, recon loss is 0.005, classification loss is 0.201\n",
      "Ran Iteration in 12.6312 seconds\n",
      "Training at Epoch 0 iteration 203, total loss is 0.301, proj loss is 0.093, recon loss is 0.005, classification loss is 0.203\n",
      "Ran Iteration in 12.7913 seconds\n",
      "Training at Epoch 0 iteration 204, total loss is 0.294, proj loss is 0.078, recon loss is 0.005, classification loss is 0.211\n",
      "Ran Iteration in 12.8256 seconds\n",
      "Training at Epoch 0 iteration 205, total loss is 0.217, proj loss is 0.058, recon loss is 0.005, classification loss is 0.154\n",
      "Ran Iteration in 12.7611 seconds\n",
      "Training at Epoch 0 iteration 206, total loss is 0.247, proj loss is 0.049, recon loss is 0.005, classification loss is 0.193\n",
      "Ran Iteration in 12.6272 seconds\n",
      "Training at Epoch 0 iteration 207, total loss is 0.329, proj loss is 0.064, recon loss is 0.005, classification loss is 0.260\n",
      "Ran Iteration in 12.7814 seconds\n",
      "Training at Epoch 0 iteration 208, total loss is 0.326, proj loss is 0.082, recon loss is 0.005, classification loss is 0.240\n",
      "Ran Iteration in 12.7818 seconds\n",
      "Training at Epoch 0 iteration 209, total loss is 0.301, proj loss is 0.093, recon loss is 0.005, classification loss is 0.202\n",
      "Ran Iteration in 12.7875 seconds\n",
      "Training at Epoch 0 iteration 210, total loss is 0.331, proj loss is 0.097, recon loss is 0.005, classification loss is 0.230\n",
      "Ran Iteration in 12.7939 seconds\n",
      "Training at Epoch 0 iteration 211, total loss is 0.284, proj loss is 0.091, recon loss is 0.005, classification loss is 0.188\n",
      "Ran Iteration in 12.6538 seconds\n",
      "Training at Epoch 0 iteration 212, total loss is 0.262, proj loss is 0.076, recon loss is 0.005, classification loss is 0.181\n",
      "Ran Iteration in 12.7737 seconds\n",
      "Training at Epoch 0 iteration 213, total loss is 0.242, proj loss is 0.054, recon loss is 0.005, classification loss is 0.183\n",
      "Ran Iteration in 12.7987 seconds\n",
      "Training at Epoch 0 iteration 214, total loss is 0.249, proj loss is 0.043, recon loss is 0.005, classification loss is 0.201\n",
      "Ran Iteration in 12.7822 seconds\n",
      "Training at Epoch 0 iteration 215, total loss is 0.317, proj loss is 0.063, recon loss is 0.005, classification loss is 0.249\n",
      "Ran Iteration in 12.8016 seconds\n",
      "Training at Epoch 0 iteration 216, total loss is 0.277, proj loss is 0.094, recon loss is 0.005, classification loss is 0.178\n",
      "Ran Iteration in 12.8016 seconds\n",
      "Training at Epoch 0 iteration 217, total loss is 0.338, proj loss is 0.118, recon loss is 0.005, classification loss is 0.215\n",
      "Ran Iteration in 12.7681 seconds\n",
      "Training at Epoch 0 iteration 218, total loss is 0.401, proj loss is 0.133, recon loss is 0.005, classification loss is 0.263\n",
      "Ran Iteration in 12.7876 seconds\n",
      "Training at Epoch 0 iteration 219, total loss is 0.321, proj loss is 0.137, recon loss is 0.005, classification loss is 0.178\n",
      "Ran Iteration in 12.6492 seconds\n",
      "Training at Epoch 0 iteration 220, total loss is 0.281, proj loss is 0.129, recon loss is 0.005, classification loss is 0.148\n",
      "Ran Iteration in 12.6475 seconds\n",
      "Training at Epoch 0 iteration 221, total loss is 0.301, proj loss is 0.112, recon loss is 0.005, classification loss is 0.184\n",
      "Ran Iteration in 12.7617 seconds\n",
      "Training at Epoch 0 iteration 222, total loss is 0.301, proj loss is 0.090, recon loss is 0.005, classification loss is 0.206\n",
      "Ran Iteration in 12.8208 seconds\n",
      "Training at Epoch 0 iteration 223, total loss is 0.303, proj loss is 0.063, recon loss is 0.005, classification loss is 0.235\n",
      "Ran Iteration in 12.7122 seconds\n",
      "Training at Epoch 0 iteration 224, total loss is 0.276, proj loss is 0.051, recon loss is 0.005, classification loss is 0.221\n",
      "Ran Iteration in 12.7909 seconds\n",
      "Training at Epoch 0 iteration 225, total loss is 0.308, proj loss is 0.071, recon loss is 0.005, classification loss is 0.232\n",
      "Ran Iteration in 12.6460 seconds\n",
      "Training at Epoch 0 iteration 226, total loss is 0.275, proj loss is 0.099, recon loss is 0.005, classification loss is 0.171\n",
      "Ran Iteration in 1.1713 seconds\n",
      "Training at Epoch 0 iteration 227, total loss is 0.289, proj loss is 0.007, recon loss is 0.005, classification loss is 0.277\n",
      "Test at Epoch 0 , AUC: 0.9776567890486431\n",
      "Ran Iteration in 12.8063 seconds\n",
      "Training at Epoch 1 iteration 0, total loss is 0.347, proj loss is 0.141, recon loss is 0.005, classification loss is 0.201\n",
      "Ran Iteration in 12.6344 seconds\n",
      "Training at Epoch 1 iteration 1, total loss is 0.300, proj loss is 0.150, recon loss is 0.005, classification loss is 0.145\n",
      "Ran Iteration in 12.6530 seconds\n",
      "Training at Epoch 1 iteration 2, total loss is 0.342, proj loss is 0.151, recon loss is 0.005, classification loss is 0.186\n",
      "Ran Iteration in 12.8100 seconds\n",
      "Training at Epoch 1 iteration 3, total loss is 0.291, proj loss is 0.139, recon loss is 0.005, classification loss is 0.147\n",
      "Ran Iteration in 12.8057 seconds\n",
      "Training at Epoch 1 iteration 4, total loss is 0.277, proj loss is 0.118, recon loss is 0.005, classification loss is 0.154\n",
      "Ran Iteration in 12.8082 seconds\n",
      "Training at Epoch 1 iteration 5, total loss is 0.265, proj loss is 0.093, recon loss is 0.005, classification loss is 0.167\n",
      "Ran Iteration in 12.7829 seconds\n",
      "Training at Epoch 1 iteration 6, total loss is 0.229, proj loss is 0.066, recon loss is 0.005, classification loss is 0.158\n",
      "Ran Iteration in 12.7809 seconds\n",
      "Training at Epoch 1 iteration 7, total loss is 0.249, proj loss is 0.043, recon loss is 0.005, classification loss is 0.201\n",
      "Ran Iteration in 12.7906 seconds\n",
      "Training at Epoch 1 iteration 8, total loss is 0.236, proj loss is 0.072, recon loss is 0.005, classification loss is 0.159\n",
      "Ran Iteration in 12.8182 seconds\n",
      "Training at Epoch 1 iteration 9, total loss is 0.323, proj loss is 0.103, recon loss is 0.005, classification loss is 0.215\n",
      "Ran Iteration in 12.8095 seconds\n",
      "Training at Epoch 1 iteration 10, total loss is 0.217, proj loss is 0.121, recon loss is 0.005, classification loss is 0.091\n",
      "Ran Iteration in 12.7912 seconds\n",
      "Training at Epoch 1 iteration 11, total loss is 0.322, proj loss is 0.128, recon loss is 0.005, classification loss is 0.189\n",
      "Ran Iteration in 12.6544 seconds\n",
      "Training at Epoch 1 iteration 12, total loss is 0.353, proj loss is 0.119, recon loss is 0.005, classification loss is 0.229\n",
      "Ran Iteration in 12.6236 seconds\n",
      "Training at Epoch 1 iteration 13, total loss is 0.312, proj loss is 0.105, recon loss is 0.005, classification loss is 0.203\n",
      "Ran Iteration in 12.6453 seconds\n",
      "Training at Epoch 1 iteration 14, total loss is 0.290, proj loss is 0.084, recon loss is 0.005, classification loss is 0.201\n",
      "Ran Iteration in 12.7660 seconds\n",
      "Training at Epoch 1 iteration 15, total loss is 0.215, proj loss is 0.061, recon loss is 0.005, classification loss is 0.148\n",
      "Ran Iteration in 12.7793 seconds\n",
      "Training at Epoch 1 iteration 16, total loss is 0.220, proj loss is 0.057, recon loss is 0.005, classification loss is 0.158\n",
      "Ran Iteration in 12.6171 seconds\n",
      "Training at Epoch 1 iteration 17, total loss is 0.219, proj loss is 0.075, recon loss is 0.005, classification loss is 0.139\n",
      "Ran Iteration in 12.7752 seconds\n",
      "Training at Epoch 1 iteration 18, total loss is 0.245, proj loss is 0.097, recon loss is 0.005, classification loss is 0.143\n",
      "Ran Iteration in 12.7549 seconds\n",
      "Training at Epoch 1 iteration 19, total loss is 0.268, proj loss is 0.110, recon loss is 0.005, classification loss is 0.153\n",
      "Ran Iteration in 12.7990 seconds\n",
      "Training at Epoch 1 iteration 20, total loss is 0.342, proj loss is 0.109, recon loss is 0.005, classification loss is 0.228\n",
      "Ran Iteration in 12.7773 seconds\n",
      "Training at Epoch 1 iteration 21, total loss is 0.269, proj loss is 0.096, recon loss is 0.005, classification loss is 0.168\n",
      "Ran Iteration in 12.7816 seconds\n",
      "Training at Epoch 1 iteration 22, total loss is 0.244, proj loss is 0.079, recon loss is 0.005, classification loss is 0.160\n",
      "Ran Iteration in 12.7765 seconds\n",
      "Training at Epoch 1 iteration 23, total loss is 0.285, proj loss is 0.058, recon loss is 0.005, classification loss is 0.222\n",
      "Ran Iteration in 12.7892 seconds\n",
      "Training at Epoch 1 iteration 24, total loss is 0.210, proj loss is 0.055, recon loss is 0.005, classification loss is 0.150\n",
      "Ran Iteration in 12.7982 seconds\n",
      "Training at Epoch 1 iteration 25, total loss is 0.311, proj loss is 0.079, recon loss is 0.005, classification loss is 0.227\n",
      "Ran Iteration in 12.8204 seconds\n",
      "Training at Epoch 1 iteration 26, total loss is 0.302, proj loss is 0.111, recon loss is 0.005, classification loss is 0.186\n",
      "Ran Iteration in 12.6471 seconds\n",
      "Training at Epoch 1 iteration 27, total loss is 0.272, proj loss is 0.126, recon loss is 0.005, classification loss is 0.141\n",
      "Ran Iteration in 12.7999 seconds\n",
      "Training at Epoch 1 iteration 28, total loss is 0.309, proj loss is 0.137, recon loss is 0.005, classification loss is 0.168\n",
      "Ran Iteration in 12.7767 seconds\n",
      "Training at Epoch 1 iteration 29, total loss is 0.272, proj loss is 0.140, recon loss is 0.005, classification loss is 0.128\n",
      "Ran Iteration in 12.7982 seconds\n",
      "Training at Epoch 1 iteration 30, total loss is 0.268, proj loss is 0.126, recon loss is 0.005, classification loss is 0.137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#  if __name__ == '__main__':\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     model_max, loss_c, loss_r, loss_p = main_dde_nn()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     pass\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_max, loss_c, loss_r, loss_p \u001b[38;5;241m=\u001b[39m \u001b[43mmain_dde_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mmain_dde_nn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    127\u001b[0m v_D \u001b[38;5;241m=\u001b[39m v_D\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 128\u001b[0m recon, code, score, Z_f, z_D \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_D\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m label \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(label))\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m    131\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n",
      "File \u001b[0;32m~/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:78\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 78\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/pygpu_nick/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/pygpu_nick/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  if __name__ == '__main__':\n",
    "#     model_max, loss_c, loss_r, loss_p = main_dde_nn()\n",
    "#     pass\n",
    "\n",
    "model_max, loss_c, loss_r, loss_p = main_dde_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss_c\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_c' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nih492/.conda/envs/pygpu_nick/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m test_set \u001b[38;5;241m=\u001b[39m supData(df_ddi\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues, labels_sup, df_ddi)\n\u001b[1;32m     11\u001b[0m test_generator_sup \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(test_set, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 13\u001b[0m model_nn \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_max\u001b[49m\n\u001b[1;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m y_label \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_max' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "params = {'batch_size': 256,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 6}\n",
    "\n",
    "dataFolder = './data'\n",
    "\n",
    "df_ddi = pd.read_csv('data/BIOSNAP/sup_test.csv')  # ddi dataframe drug1_smiles, drug2_smiles\n",
    "labels_sup = df_ddi.label.values\n",
    "test_set = supData(df_ddi.index.values, labels_sup, df_ddi)\n",
    "test_generator_sup = data.DataLoader(test_set, **params)\n",
    "\n",
    "model_nn = model_max\n",
    "\n",
    "y_pred = []\n",
    "y_label = []\n",
    "model_nn.eval()\n",
    "for i, (v_D, label) in tqdm(enumerate(test_generator_sup)):\n",
    "    recon, code, score, Z_f, z_D = model_nn(v_D.float())\n",
    "    m = torch.nn.Sigmoid()\n",
    "    logits = torch.squeeze(m(score)).detach().cpu().numpy()\n",
    "    label_ids = label.to('cpu').numpy()\n",
    "    y_label = y_label + label_ids.flatten().tolist()\n",
    "    y_pred = y_pred + logits.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8941994440870269"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aabbf972940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_score(y_label, y_pred)\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "average_precision = average_precision_score(y_label, y_pred)\n",
    "precision, recall, _ = precision_recall_curve(y_label, y_pred)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2aabd56c1f98>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aabbf9c89e8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Val (area = {:.3f})'.format(auc_score))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e97a27ba4cbc163212f54a86cf00f8c9e1e60825a02b37fa03a1da346ec81faf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
